{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "### Data Source\n",
    "\n",
    "Data was sourced from the subreddits `r/TwoSentenceHorror` and `r/TwoSentenceComedy` using the `redshift` api.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Is there a model that can accurately predicts whether a two sentence post is from the `r/TwoSentenceHorror` subreddit?\n",
    "- Can the hyperparameters of a model be used to improve the accuracy of this model?\n",
    "\n",
    "### Results\n",
    "\n",
    "The baseline accuracy was 66% of the majority class (text from `r/TwoSentenceHorror`). The un-tuned models yield the following accuracy scores and Matthews Correlation Coefficient. As we can see, all models (except the svm classifier with count vectorizer) outperform the baseline accuracy.\n",
    "\n",
    "In addition to the accuracy score, the matthews' correlation coefficient was used to score model performance. This metric is essentially tracking the correlation between the predicted and actual values. This is a value that falls between -1 and 1. A value of 0 means that the prediction is no better than a completely random prediction.\n",
    "\n",
    "|vectorizer | model | accuracy score | matthews correlation coefficient (mcc) |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "|count vectorizer | logistic regression | 70% | 26% |\n",
    "| | knn classifier | 70% | 25% |\n",
    "| | random forest classifier | 71% | 32% |\n",
    "| | adaboost | 72% | 33% |\n",
    "| | svm classifier | 34% | 5% |\n",
    "| | multinomial naive bayes | 68% | 21% |\n",
    "| tfidf | logistic regression | 76% | 43% |\n",
    "| | knn classifier | 72% | 31% |\n",
    "| | random forest classifier | 73% | 35% |\n",
    "| | adaboost | 72% | 38% |\n",
    "| | svm classifier | 77% | 46% |\n",
    "| | multinomial naive bayes | 68% | 20% |\n",
    "\n",
    "The models that were chosen for tuning were logistic regression and logistic regression, both with tfidf vectorizer. The results of the tuning was:\n",
    "\n",
    "| model | accuracy score | matthews correlation coefficient |\n",
    "| :---: | :---: | :---: |\n",
    "| logistic regression | 77% | 46% |\n",
    "| svm classifier | 76% | 44% |\n",
    "\n",
    "While the logistic regression accuracy and mcc scores improved with tuning, both scores actually dropped for svm post tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Analysis Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TwoSentenceHorror</td>\n",
       "      <td>I was watching a movie with my 5 year old son,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TwoSentenceHorror</td>\n",
       "      <td>“You know, I’ve never bungee jumped before– ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TwoSentenceHorror</td>\n",
       "      <td>I gently put down my baby in his crib before I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TwoSentenceHorror</td>\n",
       "      <td>My cat has a very annoying habit of shoving hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TwoSentenceHorror</td>\n",
       "      <td>I shuddered as I heard the screams coming from...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subreddit                                               text\n",
       "0  TwoSentenceHorror  I was watching a movie with my 5 year old son,...\n",
       "1  TwoSentenceHorror  “You know, I’ve never bungee jumped before– ma...\n",
       "2  TwoSentenceHorror  I gently put down my baby in his crib before I...\n",
       "3  TwoSentenceHorror  My cat has a very annoying habit of shoving hi...\n",
       "4  TwoSentenceHorror  I shuddered as I heard the screams coming from..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/data_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3606 entries, 0 to 3605\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  3606 non-null   object\n",
      " 1   text       3582 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.662784\n",
       "0    0.337216\n",
       "Name: is_horror, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new feature column 'is_horror'\n",
    "\n",
    "df.loc[:,'is_horror'] = df.loc[:,'subreddit'].map({\n",
    "    'TwoSentenceHorror' : 1,\n",
    "    'TwoSentenceComedy' : 0\n",
    "})\n",
    "\n",
    "df.loc[:, 'is_horror'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df.isnull().any(axis = 1)].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy\n",
    "\n",
    "Baseline accuracy (the proportion in the majority class) is about $66\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.loc[:,'text']\n",
    "y = df.loc[:,'is_horror']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(896,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Setup: word vectorizors and models\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Classification Metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Text Vectorization\n",
    "\n",
    "### Workflow\n",
    "\n",
    "We begin by first fitting\n",
    "\n",
    "Rather than re-writing each line of code repeatedly, I create an object that, when initialized, will handle text vectorization and model fitting in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization and Modelling without tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vec_n_model_pipe_fit:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, classifier):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        pipe_cvec = Pipeline([('vec', CountVectorizer()), ('clsfr', classifier)])\n",
    "        pipe_tvec = Pipeline([('vec', TfidfVectorizer()), ('clsfr', classifier)])\n",
    "        \n",
    "        self.fit_pipe_cvec = pipe_cvec.fit(X_train,y_train)\n",
    "        self.fit_pipe_tvec = pipe_tvec.fit(X_train,y_train)\n",
    "                \n",
    "        self.train_pred_cvec = self.fit_pipe_cvec.predict(X_train)\n",
    "        self.test_pred_cvec = self.fit_pipe_cvec.predict(X_val)\n",
    "        \n",
    "        self.train_pred_tvec = self.fit_pipe_tvec.predict(X_train)\n",
    "        self.test_pred_tvec = self.fit_pipe_tvec.predict(X_val)\n",
    "        \n",
    "    \n",
    "    def vec_model_test_perf(self):\n",
    "        return {'count_vec' : \n",
    "             {'accuracy' : self.fit_pipe_cvec.score(self.X_val, self.y_val),\n",
    "              'precision' : precision_score(self.y_val, self.test_pred_cvec),\n",
    "              'recall' : recall_score(self.y_val, self.test_pred_cvec),\n",
    "              'mcc' : matthews_corrcoef(self.y_val, self.test_pred_cvec),\n",
    "              'confusion_matrix' : confusion_matrix(self.y_val,self.test_pred_cvec)},\n",
    "             'tfidf' : \n",
    "             {'accuracy' : self.fit_pipe_tvec.score(self.X_val, self.y_val),\n",
    "              'precision' : precision_score(self.y_val, self.test_pred_tvec),\n",
    "              'recall' : recall_score(self.y_val, self.test_pred_tvec),\n",
    "              'mcc' : matthews_corrcoef(self.y_val, self.test_pred_tvec),\n",
    "              'confusion_matrix' : confusion_matrix(self.y_val,self.test_pred_tvec)}}\n",
    "    \n",
    "    def vec_model_train_perf(self):\n",
    "        return {'count_vec' : \n",
    "             {'accuracy' : self.fit_pipe_cvec.score(self.X_train, self.y_train),\n",
    "              'precision' : precision_score(self.y_train, self.train_pred_cvec),\n",
    "              'recall' : recall_score(self.y_train, self.train_pred_cvec),\n",
    "              'mcc' : matthews_corrcoef(self.y_train, self.train_pred_cvec),\n",
    "              'confusion_matrix' : confusion_matrix(self.y_train,self.train_pred_cvec)},\n",
    "             'tfidf' : \n",
    "             {'accuracy' : self.fit_pipe_tvec.score(self.X_train, self.y_train),\n",
    "              'precision' : precision_score(self.y_train, self.train_pred_tvec),\n",
    "              'recall' : recall_score(self.y_train, self.train_pred_tvec),\n",
    "              'mcc' : matthews_corrcoef(self.y_train, self.train_pred_tvec),\n",
    "              'confusion_matrix' : confusion_matrix(self.y_train,self.train_pred_tvec)}}\n",
    "    \n",
    "    def best_vectorizer(self, test = True):\n",
    "        if test:\n",
    "            return 'cvec' if self.vec_model_test_perf()['count_vec']['accuracy'] >  self.vec_model_test_perf()['tfidf']['accuracy'] else 'tfidf'\n",
    "        else:\n",
    "            return 'cvec' if self.vec_model_train_perf()['count_vec']['accuracy'] >  self.vec_model_train_perf()['tfidf']['accuracy'] else 'tfidf'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7233804914370812,\n",
       "  'precision': 0.7062825130052021,\n",
       "  'recall': 0.9949267192784668,\n",
       "  'mcc': 0.3537181477896348,\n",
       "  'confusion_matrix': array([[ 178,  734],\n",
       "         [   9, 1765]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.8674609084139985,\n",
       "  'precision': 0.8385864374403056,\n",
       "  'recall': 0.9898534385569335,\n",
       "  'mcc': 0.707430236965049,\n",
       "  'confusion_matrix': array([[ 574,  338],\n",
       "         [  18, 1756]], dtype=int64)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.6997767857142857,\n",
       "  'precision': 0.6934131736526946,\n",
       "  'recall': 0.9780405405405406,\n",
       "  'mcc': 0.25551809894519173,\n",
       "  'confusion_matrix': array([[ 48, 256],\n",
       "         [ 13, 579]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7600446428571429,\n",
       "  'precision': 0.7516688918558078,\n",
       "  'recall': 0.9510135135135135,\n",
       "  'mcc': 0.43362798610703973,\n",
       "  'confusion_matrix': array([[118, 186],\n",
       "         [ 29, 563]], dtype=int64)}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNClassifier with Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7691734921816828,\n",
       "  'precision': 0.7495674740484429,\n",
       "  'recall': 0.9768883878241262,\n",
       "  'mcc': 0.46783851389848624,\n",
       "  'confusion_matrix': array([[ 333,  579],\n",
       "         [  41, 1733]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7896500372300819,\n",
       "  'precision': 0.7638585770405937,\n",
       "  'recall': 0.9864712514092446,\n",
       "  'mcc': 0.525839731167524,\n",
       "  'confusion_matrix': array([[ 371,  541],\n",
       "         [  24, 1750]], dtype=int64)}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7008928571428571,\n",
       "  'precision': 0.7014925373134329,\n",
       "  'recall': 0.9527027027027027,\n",
       "  'mcc': 0.2546086043574658,\n",
       "  'confusion_matrix': array([[ 64, 240],\n",
       "         [ 28, 564]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7176339285714286,\n",
       "  'precision': 0.7153748411689962,\n",
       "  'recall': 0.9510135135135135,\n",
       "  'mcc': 0.31021194901810845,\n",
       "  'confusion_matrix': array([[ 80, 224],\n",
       "         [ 29, 563]], dtype=int64)}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB with Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7345495160089353,\n",
       "  'precision': 0.7133092078809811,\n",
       "  'recall': 1.0,\n",
       "  'mcc': 0.394519100398565,\n",
       "  'confusion_matrix': array([[ 199,  713],\n",
       "         [   0, 1774]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7531645569620253,\n",
       "  'precision': 0.727944193680755,\n",
       "  'recall': 1.0,\n",
       "  'mcc': 0.44581153114404254,\n",
       "  'confusion_matrix': array([[ 249,  663],\n",
       "         [   0, 1774]], dtype=int64)}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.6852678571428571,\n",
       "  'precision': 0.6785714285714286,\n",
       "  'recall': 0.9949324324324325,\n",
       "  'mcc': 0.20999221011088634,\n",
       "  'confusion_matrix': array([[ 25, 279],\n",
       "         [  3, 589]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.6830357142857143,\n",
       "  'precision': 0.676605504587156,\n",
       "  'recall': 0.9966216216216216,\n",
       "  'mcc': 0.20231132546708322,\n",
       "  'confusion_matrix': array([[ 22, 282],\n",
       "         [  2, 590]], dtype=int64)}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest with Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.9609084139985108,\n",
       "  'precision': 0.9765848086807538,\n",
       "  'recall': 0.963923337091319,\n",
       "  'mcc': 0.9135353504573496,\n",
       "  'confusion_matrix': array([[ 871,   41],\n",
       "         [  64, 1710]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 1.0,\n",
       "  'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'mcc': 1.0,\n",
       "  'confusion_matrix': array([[ 912,    0],\n",
       "         [   0, 1774]], dtype=int64)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7120535714285714,\n",
       "  'precision': 0.7427325581395349,\n",
       "  'recall': 0.8631756756756757,\n",
       "  'mcc': 0.3150529376824144,\n",
       "  'confusion_matrix': array([[127, 177],\n",
       "         [ 81, 511]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7321428571428571,\n",
       "  'precision': 0.7315789473684211,\n",
       "  'recall': 0.9391891891891891,\n",
       "  'mcc': 0.3538159641596086,\n",
       "  'confusion_matrix': array([[100, 204],\n",
       "         [ 36, 556]], dtype=int64)}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, AdaBoostClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7308265078183172,\n",
       "  'precision': 0.7332445628051487,\n",
       "  'recall': 0.9312288613303269,\n",
       "  'mcc': 0.3505886429680324,\n",
       "  'confusion_matrix': array([[ 311,  601],\n",
       "         [ 122, 1652]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7639612807148176,\n",
       "  'precision': 0.8028692879914984,\n",
       "  'recall': 0.8517474633596392,\n",
       "  'mcc': 0.4600942869172248,\n",
       "  'confusion_matrix': array([[ 541,  371],\n",
       "         [ 263, 1511]], dtype=int64)}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.7243303571428571,\n",
       "  'precision': 0.7260812581913499,\n",
       "  'recall': 0.9358108108108109,\n",
       "  'mcc': 0.33067790974010014,\n",
       "  'confusion_matrix': array([[ 95, 209],\n",
       "         [ 38, 554]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7321428571428571,\n",
       "  'precision': 0.7691131498470948,\n",
       "  'recall': 0.8496621621621622,\n",
       "  'mcc': 0.37637111626256814,\n",
       "  'confusion_matrix': array([[153, 151],\n",
       "         [ 89, 503]], dtype=int64)}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = vec_n_model_pipe_fit(X_train, y_train, X_val, y_val, SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.34363365599404316,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.0062006764374295375,\n",
       "  'mcc': 0.04597852774321858,\n",
       "  'confusion_matrix': array([[ 912,    0],\n",
       "         [1763,   11]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.9817572598659717,\n",
       "  'precision': 0.9731212287438289,\n",
       "  'recall': 1.0,\n",
       "  'mcc': 0.9596026797986088,\n",
       "  'confusion_matrix': array([[ 863,   49],\n",
       "         [   0, 1774]], dtype=int64)}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.vec_model_train_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count_vec': {'accuracy': 0.34375,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.006756756756756757,\n",
       "  'mcc': 0.04798698971257677,\n",
       "  'confusion_matrix': array([[304,   0],\n",
       "         [588,   4]], dtype=int64)},\n",
       " 'tfidf': {'accuracy': 0.7689732142857143,\n",
       "  'precision': 0.7542932628797886,\n",
       "  'recall': 0.964527027027027,\n",
       "  'mcc': 0.46124237327231543,\n",
       "  'confusion_matrix': array([[118, 186],\n",
       "         [ 21, 571]], dtype=int64)}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.vec_model_test_perf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(logit, 'logistic regression'), (knn_classifier, 'knn classifier'), (rf, 'random forest classifier'), (ada, 'adaboost'), (svc, 'svm classifier'), (mnb, 'multinomial naive bayes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 'tfidf'),\n",
       " ('knn classifier', 'tfidf'),\n",
       " ('random forest classifier', 'tfidf'),\n",
       " ('adaboost', 'tfidf'),\n",
       " ('svm classifier', 'tfidf'),\n",
       " ('multinomial naive bayes', 'cvec')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.best_vectorizer()) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.7600446428571429),\n",
       " ('knn classifier', 0.7176339285714286),\n",
       " ('random forest classifier', 0.7321428571428571),\n",
       " ('adaboost', 0.7321428571428571),\n",
       " ('svm classifier', 0.7689732142857143),\n",
       " ('multinomial naive bayes', 0.6830357142857143)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['tfidf']['accuracy']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.6997767857142857),\n",
       " ('knn classifier', 0.7008928571428571),\n",
       " ('random forest classifier', 0.7120535714285714),\n",
       " ('adaboost', 0.7243303571428571),\n",
       " ('svm classifier', 0.34375),\n",
       " ('multinomial naive bayes', 0.6852678571428571)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['count_vec']['accuracy']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator/vectorizer combo based on test accuracy is the SVM classifer, with an accuracy rate of about $76.9\\%$. While the worst performing is the SVM classifier with count vectorizer.\n",
    "\n",
    "The next best performing combo is the logistic regression with `tfidf`. The accuracy for this combination is $76.0\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.7516688918558078),\n",
       " ('knn classifier', 0.7153748411689962),\n",
       " ('random forest classifier', 0.7315789473684211),\n",
       " ('adaboost', 0.7691131498470948),\n",
       " ('svm classifier', 0.7542932628797886),\n",
       " ('multinomial naive bayes', 0.676605504587156)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['tfidf']['precision']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.6934131736526946),\n",
       " ('knn classifier', 0.7014925373134329),\n",
       " ('random forest classifier', 0.7427325581395349),\n",
       " ('adaboost', 0.7260812581913499),\n",
       " ('svm classifier', 1.0),\n",
       " ('multinomial naive bayes', 0.6785714285714286)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['count_vec']['precision']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator/vectorizer combo, based on test precision, is the SVM classifer with count vectorization with a precision of $100\\%$. While the worst performing is the multinomial naive bayes' classifier with tfidf.\n",
    "\n",
    "The next best performing combo is the adaboost classifier with `tfidf`. The precision for this combination is $76.9\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.9510135135135135),\n",
       " ('knn classifier', 0.9510135135135135),\n",
       " ('random forest classifier', 0.9391891891891891),\n",
       " ('adaboost', 0.8496621621621622),\n",
       " ('svm classifier', 0.964527027027027),\n",
       " ('multinomial naive bayes', 0.9966216216216216)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['tfidf']['recall']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.9780405405405406),\n",
       " ('knn classifier', 0.9527027027027027),\n",
       " ('random forest classifier', 0.8631756756756757),\n",
       " ('adaboost', 0.9358108108108109),\n",
       " ('svm classifier', 0.006756756756756757),\n",
       " ('multinomial naive bayes', 0.9949324324324325)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['count_vec']['recall']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator/vectorizer combo, based on test recall, is the multinomial naive bayes classifer with `tfidf` with a precision of $99.7\\%$. While the worst performing is the SVM classifier with count vectorization.\n",
    "\n",
    "The next best performing combo is the multinomial naive bayes classifer with count vectorization. The recall for this combination is $99.5\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.43362798610703973),\n",
       " ('knn classifier', 0.31021194901810845),\n",
       " ('random forest classifier', 0.3538159641596086),\n",
       " ('adaboost', 0.37637111626256814),\n",
       " ('svm classifier', 0.46124237327231543),\n",
       " ('multinomial naive bayes', 0.20231132546708322)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['tfidf']['mcc']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('logistic regression', 0.25551809894519173),\n",
       " ('knn classifier', 0.2546086043574658),\n",
       " ('random forest classifier', 0.3150529376824144),\n",
       " ('adaboost', 0.33067790974010014),\n",
       " ('svm classifier', 0.04798698971257677),\n",
       " ('multinomial naive bayes', 0.20999221011088634)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, estim.vec_model_test_perf()['count_vec']['mcc']) for (estim, name) in estimators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best estimator/vectorizer combo based on Matthews correlation coefficient is again the SVM classifer with tfidf, with an coefficient of about $46.1\\%$. The worst performing is again SVM, but with count vectorization, having a coefficient of less than $5\\%$.\n",
    "\n",
    "The next best performing combo is the logistic regression with `tfidf`. The accuracy for this combination is about $43.3\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Winner\n",
    "\n",
    "The combo with the best overall score is the SVM classifier with tfidf vectorization, and it appears that tfidf vectorization is the best overall text vectorizer.\n",
    "\n",
    "### Candidates for Tuning\n",
    "\n",
    "- SVM classifier with tfidf vectorization\n",
    "- Logistic regression with tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning - Tfidf Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1440 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done 1005 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=4)]: Done 1450 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=4)]: Done 1977 tasks      | elapsed: 14.5min\n",
      "[Parallel(n_jobs=4)]: Done 2584 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=4)]: Done 3273 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed: 29.4min\n",
      "[Parallel(n_jobs=4)]: Done 4893 tasks      | elapsed: 35.5min\n",
      "[Parallel(n_jobs=4)]: Done 5824 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=4)]: Done 6837 tasks      | elapsed: 49.7min\n",
      "[Parallel(n_jobs=4)]: Done 7200 out of 7200 | elapsed: 52.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('tf',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accent...\n",
       "             iid='deprecated', n_jobs=4,\n",
       "             param_grid={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
       "                         'svc__degree': [2, 3], 'svc__kernel': ['rbf', 'poly'],\n",
       "                         'tf__max_df': [0.9, 0.95],\n",
       "                         'tf__max_features': (2000, 3000, 5000),\n",
       "                         'tf__min_df': [2, 3],\n",
       "                         'tf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tf__stop_words': (None, 'english')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params_tf = {\n",
    "    'tf__max_features' : (2000, 3000, 5000),\n",
    "    'tf__max_df' : [0.9, 0.95],\n",
    "    'tf__min_df' : [2, 3],\n",
    "    'tf__stop_words' : (None,'english'),\n",
    "    'tf__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    'svc__C' : [0.01, 0.1, 1, 10, 100],\n",
    "    'svc__kernel' : ['rbf', 'poly'],\n",
    "    'svc__degree' : [2, 3]\n",
    "}\n",
    "\n",
    "pipe_sc = Pipeline([('tf', TfidfVectorizer()), ('svc', SVC())])\n",
    "\n",
    "# Instantiate SVM.\n",
    "grid_tf = GridSearchCV(pipe_sc, param_grid=pipe_params_tf, cv = 5, verbose=2, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit on training data.\n",
    "grid_tf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647006292704202"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "grid_tf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 10,\n",
       " 'svc__degree': 2,\n",
       " 'svc__kernel': 'rbf',\n",
       " 'tf__max_df': 0.9,\n",
       " 'tf__max_features': 5000,\n",
       " 'tf__min_df': 3,\n",
       " 'tf__ngram_range': (1, 2),\n",
       " 'tf__stop_words': None}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('tf',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=0.9, max_features=5000,\n",
       "                                 min_df=3, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('svc',\n",
       "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=2,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tf.best_estimator_ator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4402177873038213"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val_tf = grid_tf.best_estimator_.predict(X_val)\n",
    "matthews_corrcoef(y_val,pred_val_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622767857142857"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val,pred_val_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 648 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=5)]: Done 355 tasks      | elapsed:   44.9s\n",
      "[Parallel(n_jobs=5)]: Done 638 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=5)]: Done 1003 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=5)]: Done 1448 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=5)]: Done 1975 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=5)]: Done 2582 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=5)]: Done 3240 out of 3240 | elapsed:  7.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7572559932988584"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params_lr = {\n",
    "    'tf__max_features' : (2000, 3000, 5000),\n",
    "    'tf__max_df' : [0.9, 0.95],\n",
    "    'tf__min_df' : [2, 3],\n",
    "    'tf__stop_words' : (None,'english'),\n",
    "    'tf__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "    'lr__C' : [0.1, 1, 10],\n",
    "    'lr__penalty' : ['l1', 'l2', 'elasticnet']\n",
    "}\n",
    "\n",
    "pipe_lr = Pipeline([('tf', TfidfVectorizer()), ('lr', LogisticRegression(solver = 'saga'))])\n",
    "\n",
    "# Instantiate SVM.\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid=pipe_params_lr, cv = 5, verbose=2, n_jobs = 5)\n",
    "\n",
    "\n",
    "# Fit on training data.\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model.\n",
    "grid_lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 10,\n",
       " 'lr__penalty': 'l2',\n",
       " 'tf__max_df': 0.9,\n",
       " 'tf__max_features': 5000,\n",
       " 'tf__min_df': 3,\n",
       " 'tf__ngram_range': (1, 2),\n",
       " 'tf__stop_words': None}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_grid_lr = grid_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46182695964436876"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_val,estimator_grid_lr.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678571428571429"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val,estimator_grid_lr.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Hyperparameter tuning doesn't always yield the best results.\n",
    "- When tuned properly, even 'basic' models can perform better than more elaborate models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
